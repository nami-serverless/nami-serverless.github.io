---
layout: post
title: Introducing Nami
---

## Introduction

When events occur in batches, a wave of webhooks may overwhelm consumers, resulting in dropped data. Always-on systems must be provisioned to handle worst-case scenarios, however, this is inefficient as these resources will be idling most of the time. What is needed is an efficient and easily deployable architecture for consuming webhooks at scale.

Nami provides a simple interface for deploying this type of architecture to consume webhooks. It relies on the elasticity of serverless systems to handle bursty webhook traffic. The framework abstracts away the complexities of cloud infrastructure and effort required to deploy, configure, and choreograph all of the services that make up the Nami framework.

## Event-driven architecture

Webhooks represent an event-driven approach to development. They allow applications to take in data about events as they happen, trading control of data flow for event-driven updateand asynchronicity. Nami’s event-driven architecture embraces this trade-off.

Since the introduction of web communication protocols like HTTP in the 1980s, the dominant paradigm for communication between systems is a client-server request-driven architecture where a client, or many clients, make a request to a server. The client’s request may either be getting information from the server, or updating information on the server. In either case, the server returns a response acknowledging the request, optionally including data that the client requested. This request-response process is a synchronous method of communication in that the client must wait for the server to return a response before continuing execution.

In an event-driven architecture (EDA), events represent changes in state that have already occurred. “An event is an object or message that indicates something has happened;” it holds information about that happening, but does not contain any logic itself (1). Rather than requesting for work to be done, the consumer responds to events that are announced after they have occurred. The EDA paradigm is asynchronous in that a caller is never sitting idle, waiting for a response to be returned. In an asynchronous, event-driven model, the dichotomy of client and server loses the meaning it has in the request-driven context, so we can instead use terms such as event producer (server sending the event notification) and event consumer (server receiving the event notification).

![EDA example](../assets/eda.png)

## Webhooks
One way that the shift towards an event-driven paradigm manifests is in the use of webhooks. Webhooks are a mechanism for one system to notify another system of an event that has occurred.

Before webhooks became a popular way to move data, polling techniques were commonly used to retrieve event information from services. If your app needed information from a service, you needed to continually poll that service with HTTP requests at a certain time interval to see if they had updated information for you.

![Polling an API for Data](../assets/image12.png)

If that service does not have any updates, the resources used to contact the service were wasted. What if the service entered a state where they ceased to have updates for you? You would just be continually polling and wasting resources. Webhooks turn this situation around: “don’t call us, we’ll call you.”

When a particular event occurs and triggers a webhook, the producer, or webhook sender, calls the receiver by making an HTTP request to the URL configured for the webhook.

![webhook and event](../assets/image9.png)

Webhook data is most commonly sent via a POST request with the payload included in the request body. Webhooks can be sent as several different content types, including JSON, form-data, or XML. Once the request is received by the webhook consumer, a status 200 response is sent back to the webhook producer to let them know that the API endpoint is still listening and the message was successfully received.

Example webhook payload for Shopify inventory item created event

![webhook and event](../assets/image11.png)

A common use case for Webhooks is for real time event notification. Systems that generate a lot of events can quickly ramp up the webhook traffic to the destination. If webhook traffic grows consistently it is relatively easy to accommodate the extra load with additional infrastructure. More difficult though, is inconsistent, bursty traffic.

### Manual solution
To handle these occasional spikes in traffic, the webhook consumer app could provision additional infrastructure to handle the extra traffic. However, there are a few drawbacks to this approach. Servers are expensive and require ongoing maintenance. Further, provisioning infrastructure that will sit idle and unused much of the time is an inefficient use of resources, especially for consumers receiving webhooks in volatile patterns.

![servers to handle webhook traffic](../assets/image21.png)

This also makes the core application, sharing the same infrastructure, more vulnerable to downtime caused by problems with handling webhooks. Additionally, by nature of being event driven, the consumer cannot control the flow of data. If the flow of a webhook comes in like a firehose and generates more traffic than the application can handle, the result will be an unintentional denial-of-service (DoS) attack. This is like a crowd of people blocking the entry door of a business, making it more difficult for legitimate customers to enter, or denying them service (9).

Considering the challenges with using onsite infrastructure to handle webhooks at scale, these types of use cases fit well with auto-scaling compute services like AWS Lambda, AWS’ Function as a Service (FaaS) platform. The elastic scaling capability of Lambda allows for compute resources to scale up and down on demand.

## Serverless Computing | FaaS

Before we look more closely at AWS Lambdas, let’s take a step back and look at severless as a whole.

Serverless computing is characterized both by the absence of having to manage dedicated servers or the deployment environment, as well as the ability to bring an application up and down in response to an event. virtual machines or having to . This is in contrast to a central server model, where flow, control, and security are typically all orchestrated by the central server.  This is in contrast to IaaS and PaaS, which are platforms not designed to be brought online only to handle a brief execution.  where the user has more control over such resources. The serverless model focuses on choreography of the individual services rather than orchestration by a central authority. Choreography makes the system more flexible, amenable to change, and have better division of concerns. The tradeoff is added complexity and monitoring (10).

Functions as a Service, or FaaS, is a type of serverless computing offered by multiple cloud platforms, with popular services such as AWS Lambda, Google Cloud Functions, and Microsoft Azure Functions. In a FaaS environment, function code is uploaded to the cloud platform, which then deploys a container provisioned with the function code and other required resources necessary for the function to be successfully executed. With AWS Lambda functions, scaling is automatic, elastic to meet the demands of incoming requests, and fully managed by the provider . Compute containers are spun up only when needed, and then shut down as soon as the function has finished execution. The customer pays only for the actual execution time of the function , which can be a significant cost-savings over an always-on server.

Considering these benefits of a serverless solution, it’s clear that a managed server infrastructure wastes resources by requiring the provisioning of enough servers to handle bursty webhook traffic generated by batched events. Use cases consuming webhooks with such traffic patterns, that need only to perform a brief, simple acknowledgement of the request, lend themselves well to a serverless infrastructure.

## Real World Examples

Two examples of existing solutions that solve this problem of webhooks arriving in bursty, unpredictable traffic patterns serve as a general guide for using a primarily serverless infrastructure to solve this problem.

### Example 1
Jira, a project management software which tracks issues and bugs for software development teams offers webhooks that another internal service uses to automate that issue-tracking. The webhooks generally come in a consistent, steady flow which the system infrastructure can easily accommodate. However, at times, customers update or delete issues in bulk, causing spikes to the service’s incoming webhook traffic. Thousands of webhook requests can arrive in very short time periods. The webhook data needs to be processed at high-speed and then written to a data store without being dropped along the way. To handle this bursty traffic, they set up an AWS services architecture that is able to auto-scale appropriately to the number of incoming requests, and then slow the velocity of the data so as not to overwhelm the relational database where payloads remain for further processing.

![jira](../assets/image16.png)

### Example 2
Shopify ecommerce platform offers services to online shops including tools for payment, marketing, shipping, and customer engagement. Rewind uses Shopify webhooks to provide automated backups of shops' data in "real-time" as customers make changes to their store’s products. This webhook traffic can sometimes burst because customers can generate tens of thousands of webhooks instantly - for example, when a new inventory is imported (a "products/update" hook is posted) during a nightly bulk operation. Their solution implements a scalable architecture on AWS, utilizing a message queue to throttle the velocity of the webhook messages before they hit the data store.

![rewind](../assets/image23.png)

## Need for a framework

The engineers at Jira and Rewind spent a not insignificant amount of time setting up cloud services to handle their webhook traffic.  While a serverless infrastructure offers a solution to this problem of bursty webhook traffic, another problem still remains: Developers should not have to spend their time setting up this scaffolding and learning the intricacies and gotchas of a cloud platform just to scale for webhooks. A framework is needed so that system provisioning and deployment process can be streamlined

Working within cloud platforms is not a trivial endeavor. Even though AWS is the current industry leader in cloud computing, the complexity of AWS and its multitude of services means that the learning curve is steep to perform even simple tasks. Choreographing individual services to communicate and work in a coordinated way only compounds this problem. Each individual task performed on AWS might appear simple, however, they are usually comprised of several microtasks.
